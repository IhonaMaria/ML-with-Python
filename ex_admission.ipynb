{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af96025",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION FOR STUDENT ADMISSION\n",
    "\n",
    "In this exercise I found on GitHub (https://github.com/jdwittenauer/ipython-notebooks) we were asked to determine each applicant's chance of admission based on their results on two exams. As adiminstrators of the university department, we have historical data from previous applicants that can be used as a training set. For each training example, we have the applicant's scores on two exams and the admissions decision. To accomplish this, we're going to build a classification model that estimates the probability of admission based on the exam scores using a technique called logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b835971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exam 1</th>\n",
       "      <th>Exam 2</th>\n",
       "      <th>Admitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Exam 1     Exam 2  Admitted\n",
       "0  34.623660  78.024693         0\n",
       "1  30.286711  43.894998         0\n",
       "2  35.847409  72.902198         0\n",
       "3  60.182599  86.308552         1\n",
       "4  79.032736  75.344376         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "path = os.getcwd() + '\\data\\ex2data1.txt'\n",
    "data = pd.read_csv(path, header=None, names=['Exam 1', 'Exam 2', 'Admitted'])\n",
    "data.head() #Value of 1: The student was admitted\n",
    "            #Value of 0: The student was not admitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3fa386",
   "metadata": {},
   "source": [
    "In this case, it is not necessary to do any data pre-processing because we don't have text characters, null values or different scale problems. \n",
    "\n",
    "Once we have the hypothesis, we will need to evaluate it in order to check that it generalizes correclty and doesn't generate overfitting. Therefore, we will divide now the experimental data into three sets: the train set, the test set and the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17a3077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)  #We have a total of 100 data\n",
    "\n",
    "\n",
    "#The 60% of the data that we have will be assigned as the train set.\n",
    "#The other 40% of the data will be assigned as the test set. From this 40% of the test set, half will be assigned \n",
    "#to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f2a97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data set into the train and test set as mentioned before\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b457dcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 60 entries, 49 to 51\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Exam 1    60 non-null     float64\n",
      " 1   Exam 2    60 non-null     float64\n",
      " 2   Admitted  60 non-null     int64  \n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 1.9 KB\n"
     ]
    }
   ],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f81d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the test set: half will be the test set and the other the validation set (50/50).\n",
    "val_set, test_set = train_test_split(test_set, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb4b4d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set length: 60\n",
      "Validation Set length: 20\n",
      "Test Set length: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set length:\", len(train_set))\n",
    "print(\"Validation Set length:\", len(val_set))\n",
    "print(\"Test Set length:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeec55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_set[['Exam 1', 'Exam 2']]\n",
    "y_test=test_set['Admitted']\n",
    "\n",
    "\n",
    "x_train = train_set[['Exam 1', 'Exam 2']]\n",
    "y_train=train_set['Admitted']\n",
    "\n",
    "\n",
    "x_val = val_set[['Exam 1', 'Exam 2']]\n",
    "y_val=val_set['Admitted']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92b47694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's train our algorithm using the train data set:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7623192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's do a prediction using the generalized logistic regression model obtained and trained before.\n",
    "#We will first use the validation subset to evaluate the model. \n",
    "\n",
    "y_pred = clf.predict(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae83172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      " [1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0]\n",
      "\n",
      "Real labels:\n",
      " 39    0\n",
      "30    1\n",
      "53    0\n",
      "72    1\n",
      "88    1\n",
      "70    0\n",
      "11    0\n",
      "66    1\n",
      "45    0\n",
      "5     0\n",
      "42    1\n",
      "85    1\n",
      "18    1\n",
      "26    1\n",
      "12    1\n",
      "55    0\n",
      "80    1\n",
      "90    1\n",
      "9     1\n",
      "35    0\n",
      "Name: Admitted, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction:\\n\", y_pred)\n",
    "print(\"\\nReal labels:\\n\", y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dffe1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "#Let's get the accuracy of the result:\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: {:.3f}'.format(accuracy_score(y_val, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "#An accuracy of 100% may indicate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6f19b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try now the prediction with the test set:\n",
    "\n",
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8834feb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.700\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: {:.3f}'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "#Now we get a lower accuracy than before, which indicates that somehow there is something wrong with our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "237b9de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with the validation_set: 0.5454545454545454\n",
      "F1 score with the test_set: 0.75\n"
     ]
    }
   ],
   "source": [
    "#Let's try out f1_score, which  takes into account both precision and recall, and is better to evaluate our model.\n",
    "#It indicates us how good the alogrithm works (the closer to 1, the better)\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score  \n",
    "\n",
    "print(\"F1 score with the validation_set:\", f1_score(y_val, y_pred))\n",
    "print(\"F1 score with the test_set:\", f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e18df",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The reason we may have an accuracy of 1 in the validation set but a lower F1 score is that accuracy alone may not be the best metric to evaluate the performance of a classifier. In some cases, a model can achieve high accuracy by simply predicting the majority class all the time. This can lead to high accuracy, but the model may not be able to predict the minority class well.\n",
    "\n",
    "In this case, it's possible that the model is overfitting to the validation set, which is why it's achieving perfect accuracy but lower F1 score. The lower F1 score in the validation set suggests that the model may not be generalizing well to new, unseen data.\n",
    "\n",
    "Similarly, the reason why we may have a lower accuracy in the test set but a higher F1 score is that the model is better at predicting the minority class in the test set, which is reflected in the higher F1 score. The lower accuracy in the test set may be due to the fact that the test set may be more challenging or representative of the real-world data, as opposed to the validation set that the model has already seen during training.\n",
    "\n",
    "Overfitting occurs when the model learns the noise in the training data rather than the underlying pattern. This results in a model that performs well on the training data but poorly on new, unseen data. To address this issue, we could try to reduce the complexity of the model by using regularization techniques, increasing the size of the training set, or adjusting the hyperparameters of the model. We could also try to collect more data or use data augmentation techniques to increase the variability of the data.\n",
    "\n",
    "It's important to use a combination of metrics when evaluating the performance of a classifier, as no single metric can give a complete picture of the model's performance.\n",
    "\n",
    "A good model would have a F1 score close to 1 for both the validation and test set.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
